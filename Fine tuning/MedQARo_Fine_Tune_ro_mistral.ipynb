{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCSTeh87_lR4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlk_zL4u_vaM"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/romedqa_train_dataset.csv'\n",
        "val_path = '/content/drive/MyDrive/romedqa_val_dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2myG0ols_9y3"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_path, index_col=0)\n",
        "val_df = pd.read_csv(val_path, index_col=0)",
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyKSkgkUkhCU"
      },
      "outputs": [],
      "source": [
        "# Clean-up epicriza\n",
        "train_df[\"Epicriza\"] = train_df[\"Epicriza\"].apply(lambda x: str(x).strip().replace('\\n', '').replace('\\r', ''))\n",
        "val_df[\"Epicriza\"] = val_df[\"Epicriza\"].apply(lambda x: str(x).strip().replace('\\n', '').replace('\\r', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEfSbAA4QHas"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    # raise SystemError('GPU device not found')\n",
        "    print(\"GPU not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYsV4H8fCpZ-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB7Z5a7BeZzu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForQuestionAnswering\n",
        "\n",
        "model_name = \"OpenLLM-Ro/RoMistral-7b-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model_qa = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShY1vJkVAWs9"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYFy_oQcAL3U"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "\n",
        "datasets = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(val_df),\n",
        "})\n",
        "\n",
        "\n",
        "datasets['train'] = datasets['train'].shuffle(seed=42)\n",
        "datasets['validation'] = datasets['validation'].shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZf9VtnbplzJ"
      },
      "outputs": [],
      "source": [
        "# !pip install peft==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ower57hhYa2k"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.5,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model_qa, peft_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9FNUMrvr8S4"
      },
      "outputs": [],
      "source": [
        "def truncate_context(text, tokenizer, max_tokens):\n",
        "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
        "    return tokenizer.decode(tokens, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCgB7pygaYzB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "MAX_CONTEXT_TOKENS = 2048\n",
        "\n",
        "def preprocess(example):\n",
        "    truncated_context = truncate_context(example[\"Epicriza\"], tokenizer, MAX_CONTEXT_TOKENS - 100)\n",
        "\n",
        "    prompt = f\"Intrebare: {example['Intrebare']} Context: {truncated_context} Raspuns:\"\n",
        "    full_text = prompt + \" \" + example[\"Raspuns\"]\n",
        "\n",
        "    tokenized = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=MAX_CONTEXT_TOKENS + 100)\n",
        "\n",
        "    prompt_len = len(tokenizer(prompt, truncation=True, padding=False)[\"input_ids\"])\n",
        "    labels = [-100] * prompt_len + tokenized[\"input_ids\"][prompt_len:]\n",
        "    labels = labels[:len(tokenized[\"input_ids\"])]\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "\n",
        "tokenized_datasets = datasets.map(preprocess, remove_columns=[\"Epicriza\", \"Intrebare\", \"Raspuns\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx9ThdWYIftc"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4r6axvtBBjfd"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    output_dir=\"/content/drive/MyDrive/ro-mistral-finetuned-2048\",\n",
        "    num_train_epochs=5,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    gradient_accumulation_steps=8,\n",
        "    eval_accumulation_steps=1,\n",
        "    label_names=[\"labels\"],\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
