{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCSTeh87_lR4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlk_zL4u_vaM"
      },
      "outputs": [],
      "source": [
        "val_path = '/content/drive/MyDrive/romedqa_val_dataset.csv'\n",
        "test_path = '/content/drive/MyDrive/romedqa_test_dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2myG0ols_9y3"
      },
      "outputs": [],
      "source": [
        "val_df = pd.read_csv(val_path, index_col=0)\n",
        "test_df = pd.read_csv(test_path, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyKSkgkUkhCU"
      },
      "outputs": [],
      "source": [
        "val_df[\"Epicriza\"] = val_df[\"Epicriza\"].apply(lambda x: str(x).strip().replace('\\n', '').replace('\\r', ''))\n",
        "test_df[\"Epicriza\"] = test_df[\"Epicriza\"].apply(lambda x: str(x).strip().replace('\\n', '').replace('\\r', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEfSbAA4QHas"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    # raise SystemError('GPU device not found')\n",
        "    print(\"GPU not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYsV4H8fCpZ-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShY1vJkVAWs9"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYFy_oQcAL3U"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "\n",
        "datasets = DatasetDict({\n",
        "    \"validation\": Dataset.from_pandas(val_df),\n",
        "    \"test\": Dataset.from_pandas(test_df)\n",
        "})\n",
        "\n",
        "\n",
        "datasets['validation'] = datasets['validation'].shuffle(seed=42)\n",
        "datasets['test'] = datasets['test'].shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt0zs32hHhcH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForQuestionAnswering\n",
        "\n",
        "model_name = \"/content/drive/MyDrive/phi-4-finetuned-2048/checkpoint-8971\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model_qa = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9FNUMrvr8S4"
      },
      "outputs": [],
      "source": [
        "def truncate_context(text, tokenizer, max_tokens):\n",
        "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
        "    return tokenizer.decode(tokens, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCgB7pygaYzB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "MAX_CONTEXT_TOKENS = 2048\n",
        "\n",
        "def preprocess_eval(example):\n",
        "    truncated_context = truncate_context(example[\"Epicriza\"], tokenizer, MAX_CONTEXT_TOKENS - 100)\n",
        "\n",
        "    prompt = f\"Intrebare: {example['Intrebare']} Context: {truncated_context} Raspuns:\"\n",
        "\n",
        "    entry = {}\n",
        "    entry[\"prompt_text\"] = prompt\n",
        "    entry[\"gold_answer\"] = example[\"Raspuns\"]\n",
        "    return entry\n",
        "\n",
        "tokenized_datasets = datasets.map(preprocess_eval, remove_columns=[\"Epicriza\", \"Intrebare\", \"Raspuns\", \"__index_level_0__\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcu51x5V0vZn"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUW0DGY8nmUK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import evaluate\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load metric evaluators\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "meteor_metric = evaluate.load(\"meteor\")\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "def compute_f1(prediction, ground_truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(ground_truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    num_same = len(common)\n",
        "\n",
        "    if num_same == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(truth_tokens)\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def compute_em(prediction, ground_truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(ground_truth))\n",
        "\n",
        "def compute_metrics(predicted_text, ground_truth):\n",
        "  smoothie = SmoothingFunction().method4\n",
        "\n",
        "  pred = predicted_text\n",
        "  label = ground_truth\n",
        "\n",
        "  pred_tokens = word_tokenize(pred)\n",
        "  label_tokens = word_tokenize(label)\n",
        "  bleu = sentence_bleu([label_tokens], pred_tokens, smoothing_function=smoothie)\n",
        "  meteor_score = meteor_metric.compute(predictions=[pred], references=[label])[\"meteor\"]\n",
        "  return {\n",
        "      \"f1\": compute_f1(pred, label),\n",
        "      \"exact_match\": compute_em(pred,label),\n",
        "      \"bleu\": bleu,\n",
        "      \"meteors\": meteor_score,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI0huNlufcTy"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # pentru tokenizarea Ã®n cuvinte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnniIiHYbJQ7"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "NHHB5YpGxUJU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "total_ind = []\n",
        "total_labels = []\n",
        "f1 = 0\n",
        "exact = 0\n",
        "bleu = 0\n",
        "meteor = 0\n",
        "\n",
        "\n",
        "for index in tqdm(range(len(tokenized_datasets['validation']))):\n",
        "  gold_answer =  tokenized_datasets['validation'][index]['gold_answer']\n",
        "  inputs = tokenizer(tokenized_datasets['validation'][index]['prompt_text'], return_tensors=\"pt\").to(model_qa.device)\n",
        "  prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "  outputs = model_qa.generate(**inputs, max_new_tokens=10)\n",
        "\n",
        "  generated_ids = outputs[0][prompt_len:]\n",
        "  generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "  results = compute_metrics(generated_text, gold_answer)\n",
        "  f1 += results[\"f1\"]\n",
        "  exact += results[\"exact_match\"]\n",
        "  bleu += results[\"bleu\"]\n",
        "  meteor += results[\"meteors\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "03tOnpmtJJ9L"
      },
      "outputs": [],
      "source": [
        "print(\"ðŸ“Š Evaluare pe setul de validare:\")\n",
        "print(f\"ðŸ”¹ F1 Score       : {f1/len(tokenized_datasets['validation']):.4f}\")\n",
        "print(f\"ðŸ”¹ Exact Match    : {exact/len(tokenized_datasets['validation']):.4f}\")\n",
        "print(f\"ðŸ”¹ BLEU Score     : {bleu/len(tokenized_datasets['validation']):.4f}\")\n",
        "print(f\"ðŸ”¹ Meteor Score     : {meteor/len(tokenized_datasets['validation']):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pWqiP4LI6v4S"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "total_ind = []\n",
        "total_labels = []\n",
        "f1 = 0\n",
        "exact = 0\n",
        "bleu = 0\n",
        "meteor = 0\n",
        "\n",
        "for index in tqdm(range(len(tokenized_datasets['test']))):\n",
        "  gold_answer =  tokenized_datasets['test'][index]['gold_answer']\n",
        "  inputs = tokenizer(tokenized_datasets['test'][index]['prompt_text'], return_tensors=\"pt\").to(model_qa.device)\n",
        "  prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "  outputs = model_qa.generate(**inputs, max_new_tokens=10)\n",
        "\n",
        "  generated_ids = outputs[0][prompt_len:]  # only new tokens\n",
        "  generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "  results = compute_metrics(generated_text, gold_answer)\n",
        "  f1 += results[\"f1\"]\n",
        "  exact += results[\"exact_match\"]\n",
        "  bleu += results[\"bleu\"]\n",
        "  meteor += results[\"meteors\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "41gx-bQ8nlF9"
      },
      "outputs": [],
      "source": [
        "print(\"ðŸ“Š Evaluare pe setul de test:\")\n",
        "print(f\"ðŸ”¹ F1 Score       : {f1/len(tokenized_datasets['test']):.4f}\")\n",
        "print(f\"ðŸ”¹ Exact Match    : {exact/len(tokenized_datasets['test']):.4f}\")\n",
        "print(f\"ðŸ”¹ BLEU Score     : {bleu/len(tokenized_datasets['test']):.4f}\")\n",
        "print(f\"ðŸ”¹ Meteor Score     : {meteor/len(tokenized_datasets['test']):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}